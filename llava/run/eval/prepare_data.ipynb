{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据已成功保存到 ../../data/eval/Chest-X-ray.jsonl 文件中。\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "dataset = \"Chest-X-ray\"\n",
    "\n",
    "# convert csv to jsonl\n",
    "data = pd.read_csv(f\"../../data/eval/{dataset}.csv\")\n",
    "\n",
    "# 将DataFrame转换为字典列表\n",
    "data_list = data.to_dict(orient='records')\n",
    "\n",
    "# 固定随机种子并打乱数据\n",
    "random.seed(1234)\n",
    "random.shuffle(data_list)\n",
    "split_point = int(len(data_list) * 0.75)\n",
    "\n",
    "new_data = []\n",
    "for idx, item in enumerate(data_list):\n",
    "    # 提取img_path和Finding Label\n",
    "    img_path = item.get(\"img_path\")\n",
    "    finding_label = item.get(\"Finding Label\").lower()\n",
    "\n",
    "    if isinstance(finding_label, str):\n",
    "        finding_label = finding_label.replace('|', ', ')\n",
    "        \n",
    "    # 创建新的数据项\n",
    "    new_item = {\n",
    "        \"image\": img_path,\n",
    "        \"label\": finding_label,\n",
    "        \"split\": \"train\" if idx < split_point else \"test\" \n",
    "    }\n",
    "    \n",
    "    # 添加到new_data列表中\n",
    "    new_data.append(new_item)\n",
    "random.shuffle(new_data)\n",
    "# 将数据保存为JSONL文件\n",
    "jsonl_file = f\"../../data/eval/{dataset}.jsonl\"\n",
    "with open(jsonl_file, mode='w', encoding='utf-8') as f:\n",
    "    for item in new_data:\n",
    "        f.write(json.dumps(item) + \"\\n\")\n",
    "\n",
    "print(f\"数据已成功保存到 {jsonl_file} 文件中。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels saved to ../../data/eval/Chest-X-ray_classes.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "dataset = \"Chest-X-ray\"\n",
    "labels = set()\n",
    "\n",
    "jsonl_file = f\"../../data/eval/{dataset}.jsonl\"\n",
    "with open(jsonl_file, mode='w', encoding='utf-8') as f:\n",
    "    for item in new_data:\n",
    "        label = item.get(\"label\")\n",
    "        if label:  # 如果 label 存在\n",
    "            split_labels = label.split(\",\")  # 根据逗号拆分标签\n",
    "            split_labels = [lbl.strip() for lbl in split_labels]  # 去掉每个类别的前后空格\n",
    "            labels.update(split_labels)  # 更新到集合中\n",
    "\n",
    "# 将集合转换为列表并保存为 JSON 文件\n",
    "labels_list = list(labels)\n",
    "output_file = f\"../../data/eval/{dataset}_classes.json\"\n",
    "\n",
    "with open(output_file, mode='w', encoding='utf-8') as f:\n",
    "    json.dump(labels_list, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Labels saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据已成功保存到 ../data/eval/test_prompt/Chest-X-ray_llava_val.jsonl 文件中。\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "dataset = \"Chest-X-ray\"\n",
    "\n",
    "data = [json.loads(line) for line in open(f\"../data/eval/{dataset}.jsonl\")]\n",
    "\n",
    "\n",
    "new_data = []\n",
    "for idx, item in enumerate(data):\n",
    "    new_item = {\n",
    "        \"image\": item[\"image\"].replace(\"/srv/lby/\", \"\"),\n",
    "        \"text\": \"What disease is indicated by the chest X-ray?\", \n",
    "        \"category\": \"conv\",\n",
    "        \"label\": item[\"label\"],\n",
    "        \"question_id\": f'{idx}-{item[\"label\"]}',\n",
    "    }\n",
    "    new_data.append(new_item)\n",
    "\n",
    "random.shuffle(new_data)\n",
    "\n",
    "jsonl_file = f\"../data/eval/test_prompt/{dataset}_llava_val.jsonl\"\n",
    "with open(jsonl_file, mode='w', encoding='utf-8') as f:\n",
    "    for item in new_data:\n",
    "        f.write(json.dumps(item) + \"\\n\")\n",
    "        \n",
    "print(f\"数据已成功保存到 {jsonl_file} 文件中。\")\n",
    "\n",
    "\n",
    "# You are now acting as a knowledgeable radiologist. Please analyze the provided medical image and identify the most appropriate disease category or categories present in the patient. The diagnosis can involve one or more conditions. You must select the relevant categories from the following list: ['atelectasis', 'cardiomegaly', 'pleural effusion', 'infiltration', 'lung mass', 'lung nodule', 'pneumonia', 'pneumothorax', 'consolidation', 'edema', 'emphysema', 'fibrosis', 'pleural thicken', 'hernia', 'no finding']. Remember, you should only output the categories from the list, and no additional content.\n",
    "## 只输出类别的prompt\n",
    "# You are now acting as a knowledgeable radiologist. Please analyze the provided medical image and identify the most appropriate disease category or categories present in the patient. You can only output the corresponding index number of the disease in the list from 0 to 14 from the given list: ['atelectasis', 'cardiomegaly', 'pleural effusion', 'infiltration', 'lung mass', 'lung nodule', 'pneumonia', 'pneumothorax', 'consolidation', 'edema', 'emphysema', 'fibrosis', 'pleural thicken', 'hernia', 'no finding'].\n",
    "##ABCD尝试 \n",
    "# What type of disease is shown in this chest x-ray image? Choose one from A. Atelectasis.\\n B. Cardiomegaly.\\n C. Pleural Effusion.\\nD. Infiltration.\n",
    "# ['fibrosis', 'edema', 'pneumothorax', 'cardiomegaly', 'atelectasis', 'nodule', 'emphysema', 'no finding', 'mass', 'pleural_thickening', 'effusion', 'infiltration', 'pneumonia', 'hernia', 'consolidation']\n",
    "# \"text\": \"What type of disease is shown in this chest x-ray image? Choose one from A. fibrosis.\\n B. edema.\\n C. pneumothorax.\\nD. cardiomegaly. \\nE. atelectasis \\nF. nodule \\nG. emphysema \\nH. no finding \\nI. mass \\nJ. pleural thickening \\nK. effusion \\nL. infiltration \\nM. pneumonia \\nN. hernia \\nO. consolidation\", \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item 1: {'id': 0, 'image': 'p10/p10000032/s50414267/02aa804e-bde0afdd-112c0b34-7bc16630-4e384014.jpg', 'conversations': [{'from': 'human', 'value': 'What disease is indicated by the chest X-ray?\\n<image>'}, {'from': 'gpt', 'value': 'This is a chest X-ray showing Normal'}]}\n",
      "Item 2: {'id': 1, 'image': 'p10/p10000032/s53189527/2a2277a9-b0ded155-c0de8eb9-c124d10e-82c5caab.jpg', 'conversations': [{'from': 'human', 'value': 'What disease is indicated by the chest X-ray?\\n<image>'}, {'from': 'gpt', 'value': 'This is a chest X-ray showing Normal'}]}\n",
      "Item 3: {'id': 2, 'image': 'p10/p10000032/s53911762/68b5c4b1-227d0485-9cc38c3f-7b84ab51-4b472714.jpg', 'conversations': [{'from': 'human', 'value': 'What disease is indicated by the chest X-ray?\\n<image>'}, {'from': 'gpt', 'value': 'This is a chest X-ray showing Normal'}]}\n",
      "Item 4: {'id': 3, 'image': 'p10/p10000032/s53911762/fffabebf-74fd3a1f-673b6b41-96ec0ac9-2ab69818.jpg', 'conversations': [{'from': 'human', 'value': 'What disease is indicated by the chest X-ray?\\n<image>'}, {'from': 'gpt', 'value': 'This is a chest X-ray showing Normal'}]}\n",
      "Item 5: {'id': 4, 'image': 'p10/p10000032/s56699142/ea030e7a-2e3b1346-bc518786-7a8fd698-f673b44c.jpg', 'conversations': [{'from': 'human', 'value': 'What disease is indicated by the chest X-ray?\\n<image>'}, {'from': 'gpt', 'value': 'This is a chest X-ray showing Normal'}]}\n",
      "Item 6: {'id': 5, 'image': 'p10/p10000764/s57375967/096052b7-d256dc40-453a102b-fa7d01c6-1b22c6b4.jpg', 'conversations': [{'from': 'human', 'value': 'What disease is indicated by the chest X-ray?\\n<image>'}, {'from': 'gpt', 'value': 'This is a chest X-ray showing Consolidation'}]}\n",
      "Item 7: {'id': 6, 'image': 'p10/p10000898/s50771383/2a280266-c8bae121-54d75383-cac046f4-ca37aa16.jpg', 'conversations': [{'from': 'human', 'value': 'What disease is indicated by the chest X-ray?\\n<image>'}, {'from': 'gpt', 'value': 'This is a chest X-ray showing Normal'}]}\n",
      "Item 8: {'id': 7, 'image': 'p10/p10000898/s54205396/b75df1bd-0f22d631-52d73526-2ae7b85a-d843b39d.jpg', 'conversations': [{'from': 'human', 'value': 'What disease is indicated by the chest X-ray?\\n<image>'}, {'from': 'gpt', 'value': 'This is a chest X-ray showing Normal'}]}\n",
      "Item 9: {'id': 8, 'image': 'p10/p10000935/s50578979/d0b71acc-b5a62046-bbb5f6b8-7b173b85-65cdf738.jpg', 'conversations': [{'from': 'human', 'value': 'What disease is indicated by the chest X-ray?\\n<image>'}, {'from': 'gpt', 'value': 'This is a chest X-ray showing Pleural Effusion, Pneumonia'}]}\n",
      "Item 10: {'id': 9, 'image': 'p10/p10000935/s51178377/9b314ad7-fbcb0422-6db62dfc-732858d0-a5527d8b.jpg', 'conversations': [{'from': 'human', 'value': 'What disease is indicated by the chest X-ray?\\n<image>'}, {'from': 'gpt', 'value': 'This is a chest X-ray showing Lung Opacity'}]}\n",
      "Item 11: {'id': 10, 'image': 'p10/p10000935/s55697293/c50494f1-90e2bff5-e9189550-1a4562fd-6ab5204c.jpg', 'conversations': [{'from': 'human', 'value': 'What disease is indicated by the chest X-ray?\\n<image>'}, {'from': 'gpt', 'value': 'This is a chest X-ray showing Normal'}]}\n",
      "Item 12: {'id': 11, 'image': 'p10/p10000935/s56164612/8e3f2822-0c1d4b71-2a265bbf-5b96e531-ccf5fa30.jpg', 'conversations': [{'from': 'human', 'value': 'What disease is indicated by the chest X-ray?\\n<image>'}, {'from': 'gpt', 'value': 'This is a chest X-ray showing Lung Lesion, Lung Opacity'}]}\n",
      "Item 13: {'id': 12, 'image': 'p10/p10000935/s58219844/88498b37-c21dc7ba-bc202800-b517a62d-f7ac5bcf.jpg', 'conversations': [{'from': 'human', 'value': 'What disease is indicated by the chest X-ray?\\n<image>'}, {'from': 'gpt', 'value': 'This is a chest X-ray showing Normal'}]}\n",
      "Item 14: {'id': 13, 'image': 'p10/p10000980/s50985099/6ad03ed1-97ee17ee-9cf8b320-f7011003-cd93b42d.jpg', 'conversations': [{'from': 'human', 'value': 'What disease is indicated by the chest X-ray?\\n<image>'}, {'from': 'gpt', 'value': 'This is a chest X-ray showing Normal'}]}\n",
      "Item 15: {'id': 14, 'image': 'p10/p10000980/s51967283/943486a3-b3fa9ff7-50f5a769-7a62fcbb-f39b6da4.jpg', 'conversations': [{'from': 'human', 'value': 'What disease is indicated by the chest X-ray?\\n<image>'}, {'from': 'gpt', 'value': 'This is a chest X-ray showing Lung Lesion, Pneumonia'}]}\n",
      "Item 16: {'id': 15, 'image': 'p10/p10000980/s54577367/cfb03587-782edf6c-1bf392e1-98196cd5-365d69e8.jpg', 'conversations': [{'from': 'human', 'value': 'What disease is indicated by the chest X-ray?\\n<image>'}, {'from': 'gpt', 'value': 'This is a chest X-ray showing Normal'}]}\n",
      "Item 17: {'id': 16, 'image': 'p10/p10000980/s54935705/6ad819bb-bae74eb9-7b663e90-b8deabd7-57f8054a.jpg', 'conversations': [{'from': 'human', 'value': 'What disease is indicated by the chest X-ray?\\n<image>'}, {'from': 'gpt', 'value': 'This is a chest X-ray showing Edema'}]}\n",
      "Item 18: {'id': 17, 'image': 'p10/p10000980/s54980801/a75a1fbe-802065ad-717eb7c1-e2ce3552-646276a6.jpg', 'conversations': [{'from': 'human', 'value': 'What disease is indicated by the chest X-ray?\\n<image>'}, {'from': 'gpt', 'value': 'This is a chest X-ray showing Normal'}]}\n",
      "Item 19: {'id': 18, 'image': 'p10/p10000980/s57861150/5aa15ba6-55f5e96e-39cea686-7c3b28b2-b8c97a88.jpg', 'conversations': [{'from': 'human', 'value': 'What disease is indicated by the chest X-ray?\\n<image>'}, {'from': 'gpt', 'value': 'This is a chest X-ray showing Pleural Effusion'}]}\n",
      "Item 20: {'id': 19, 'image': 'p10/p10000980/s58206436/54affd39-8bf24209-232bac8a-df6c277a-398ee8a5.jpg', 'conversations': [{'from': 'human', 'value': 'What disease is indicated by the chest X-ray?\\n<image>'}, {'from': 'gpt', 'value': 'This is a chest X-ray showing Cardiomegaly, Edema, Pleural Effusion'}]}\n"
     ]
    }
   ],
   "source": [
    "# 随机遍历jsonl文件1000行，获取每一行的question_id和text内容，question_id\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, accuracy_score\n",
    "import numpy as np\n",
    "import torch\n",
    "import json\n",
    "import random\n",
    "\n",
    "# jsonl_file = \"/home/lby/llava_med/LLaVA-Med/llava/run/data/train/sft_data/modified_file.json\"\n",
    "jsonl_file = \"/home/lby/llava_med/LLaVA-Med/llava/run/data/train/sft_data/classify_mimic_file_clip.json\"\n",
    "with open(jsonl_file, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "        \n",
    "for i, item in enumerate(data[:20]):\n",
    "    print(f\"Item {i + 1}: {item}\")\n",
    "\n",
    "\n",
    "# import csv\n",
    "\n",
    "# csv_file = \"/srv/lby/llava_med/other_data/cleaned_file.csv\"\n",
    "\n",
    "# # 打开CSV文件\n",
    "# with open(csv_file, 'r', encoding='utf-8') as f:\n",
    "#     reader = csv.DictReader(f)  # 使用DictReader读取CSV文件，这样每一行会被解析成字典\n",
    "\n",
    "#     # 遍历前20行数据\n",
    "#     for i, item in enumerate(reader):\n",
    "#         if i >= 20:\n",
    "#             break  # 只处理前20行\n",
    "#         print(f\"Item {i + 1}: {item}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of answers: 1044\n",
      "Total labels: 1000\n",
      "Class distribution in ground truth: [ 13.  42.  96.  45. 134.  66.  37. 365.  64.  43. 191. 267.  28.   5.\n",
      "  70.]\n",
      "Class distribution in predictions: [296. 227.   0. 113.   0.   0.   2. 302. 421.   0.   3.  58. 320.   0.\n",
      "   0.]\n",
      "Average AUROC: 0.5012\n",
      "Average F1: 0.1736\n",
      "Average Accuracy: 0.8067\n",
      "Number of errors: 0\n",
      "Error question IDs: []\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, accuracy_score\n",
    "import numpy as np\n",
    "import torch\n",
    "import json\n",
    "import random\n",
    "\n",
    "def compute_AUCs(gt, pred, n_class):\n",
    "    \"\"\"计算每个标签的 AUC\"\"\"\n",
    "    AUROCs = []\n",
    "    gt_np = gt.cpu().numpy()\n",
    "    pred_np = pred.cpu().numpy()\n",
    "    for i in range(n_class):\n",
    "        try:\n",
    "            AUROCs.append(roc_auc_score(gt_np[:, i], pred_np[:, i]))\n",
    "        except ValueError:\n",
    "            AUROCs.append(np.nan)  # 当 AUC 计算失败时，填充 NaN\n",
    "    return AUROCs\n",
    "\n",
    "def get_acc_and_auc():\n",
    "    # 读取数据\n",
    "    output_path = '../data/eval/test_prompt/Chest-X-ray_llava_val_ans.jsonl'\n",
    "    answers = [json.loads(line) for line in open(output_path)]\n",
    "\n",
    "    disease_list = ['fibrosis', 'edema', 'pneumothorax', 'cardiomegaly', 'atelectasis', 'nodule', 'emphysema', 'no finding', 'mass', 'pleural_thickening', 'effusion', 'infiltration', 'pneumonia', 'hernia', 'consolidation']\n",
    "    print(f\"Total number of answers: {len(answers)}\")\n",
    "    \n",
    "    # 映射字典：将A, B, C, D...映射到疾病列表\n",
    "    label_mapping = {chr(65 + i): disease for i, disease in enumerate(disease_list)}\n",
    "    # 手动映射两个列表中的疾病\n",
    "\n",
    "    # 随机选择 1000 行\n",
    "    random.shuffle(answers)\n",
    "    selected_answers = answers[:1000]\n",
    "\n",
    "    # 初始化 ground truth (gt) 和 predictions (pred)\n",
    "    n_classes = len(disease_list)\n",
    "    gt = torch.zeros((len(selected_answers), n_classes), dtype=torch.float32)\n",
    "    pred = torch.zeros((len(selected_answers), n_classes), dtype=torch.float32)\n",
    "\n",
    "    error_count = 0\n",
    "    error_question_ids = []\n",
    "\n",
    "    # 遍历每个 answer，提取 labels 和预测类别\n",
    "    for idx, item in enumerate(selected_answers):\n",
    "        # 获取标签（label），labels 可能包含多个标签\n",
    "        labels = [\"-\".join(item[\"question_id\"].split(\"-\")[1:])]  # 获取 label\n",
    "        labels = [label.lower() for label in labels]\n",
    "\n",
    "        # 获取预测的 text 并映射到疾病\n",
    "        text = item[\"text\"].strip().upper()\n",
    "\n",
    "        # 设置 ground truth\n",
    "        for label in labels:\n",
    "            for i, disease in enumerate(disease_list):\n",
    "                if disease in label:\n",
    "                    gt[idx, i] = 1  # 对应疾病的 ground truth 置为 1\n",
    "\n",
    "        # 获取预测的类别 (A, B, C, D...)\n",
    "        for char in text:\n",
    "            if char in label_mapping:\n",
    "                disease = label_mapping[char]\n",
    "                disease_idx = disease_list.index(disease)\n",
    "                pred[idx, disease_idx] = 1  # 对应疾病的预测值置为 1\n",
    "\n",
    "        # 检查是否没有预测到任何类别\n",
    "        if torch.sum(pred[idx]) == 0:\n",
    "            error_count += 1\n",
    "            error_question_ids.append(item[\"question_id\"])\n",
    "\n",
    "    # 计算多标签 AUC\n",
    "    AUROCs = compute_AUCs(gt, pred, n_classes)\n",
    "    AUROC_avg = np.nanmean(AUROCs)  # 计算 AUC 的平均值，忽略 NaN\n",
    "\n",
    "    # 计算每个类别的准确率\n",
    "    accs = []\n",
    "    gt_np_all = gt.cpu().numpy()\n",
    "    pred_np_all = pred.cpu().numpy()\n",
    "\n",
    "    for i in range(n_classes):\n",
    "        gt_np = gt_np_all[:, i]\n",
    "        pred_np = pred_np_all[:, i]\n",
    "        \n",
    "        # print(f\"Class {i} - Ground truth (sample): {gt_np[:10]}\")\n",
    "        # print(f\"Class {i} - Predictions (sample): {pred_np[:10]}\")\n",
    "        acc = accuracy_score(gt_np, pred_np)\n",
    "        accs.append(acc)\n",
    "    \n",
    "    # 计算类别平均准确率\n",
    "    acc_avg = np.mean(accs)\n",
    "\n",
    "    # 计算 F1 分数\n",
    "    f1s = []\n",
    "\n",
    "    for i in range(n_classes):\n",
    "        gt_np = gt_np_all[:, i]\n",
    "        pred_np = pred_np_all[:, i]\n",
    "\n",
    "        precision, recall, thresholds = precision_recall_curve(gt_np, pred_np)\n",
    "        if len(precision) > 1:\n",
    "            numerator = 2 * recall * precision\n",
    "            denom = recall + precision\n",
    "            f1_scores = np.divide(numerator, denom, out=np.zeros_like(denom), where=(denom != 0))\n",
    "            max_f1 = np.max(f1_scores)\n",
    "            f1s.append(max_f1)\n",
    "        else:\n",
    "            f1s.append(np.nan)\n",
    "\n",
    "    f1_avg = np.nanmean(f1s)  # 平均 F1\n",
    "\n",
    "    # 输出结果\n",
    "    print(f\"Total labels: {len(selected_answers)}\")\n",
    "    print(f\"Class distribution in ground truth: {np.sum(gt_np_all, axis=0)}\")\n",
    "    print(f\"Class distribution in predictions: {np.sum(pred_np_all, axis=0)}\")\n",
    "    print(f\"Average AUROC: {AUROC_avg:.4f}\")\n",
    "    print(f\"Average F1: {f1_avg:.4f}\")\n",
    "    print(f\"Average Accuracy: {acc_avg:.4f}\")\n",
    "    print(f\"Number of errors: {error_count}\")\n",
    "    print(f\"Error question IDs: {error_question_ids}\")\n",
    "\n",
    "# 调用函数进行计算\n",
    "get_acc_and_auc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of answers: 306\n",
      "fibrosis: 0.9935\n",
      "edema: 0.9902\n",
      "pneumothorax: 0.8235\n",
      "cardiomegaly: 0.9837\n",
      "atelectasis: 0.0621\n",
      "nodule: 0.9739\n",
      "emphysema: 0.9967\n",
      "no finding: 0.6340\n",
      "mass: 0.7614\n",
      "pleural_thickening: 0.9869\n",
      "effusion: 0.0621\n",
      "infiltration: 0.0980\n",
      "pneumonia: 0.9967\n",
      "hernia: 0.9935\n",
      "consolidation: 0.8562\n",
      "Predicted as 0 for the following diseases: ['fibrosis', 'edema', 'emphysema', 'no finding', 'pleural_thickening', 'pneumonia', 'hernia']\n",
      "Total labels: 306\n",
      "Class distribution in ground truth: [  2.   3.  12.   1.   8.   7.   1. 112.  11.   4.  18.  29.   1.   2.\n",
      "   4.]\n",
      "Class distribution in predictions: [  0.   0.  48.   6. 293.   1.   0.   0.  70.   0. 305. 305.   0.   0.\n",
      "  42.]\n",
      "Average AUROC: 0.5418\n",
      "Average F1: 0.1019\n",
      "Average Accuracy: 0.7475\n",
      "Number of errors: 0\n",
      "Error question IDs: []\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, accuracy_score\n",
    "import numpy as np\n",
    "import torch\n",
    "import json\n",
    "import random\n",
    "\n",
    "def compute_AUCs(gt, pred, n_class):\n",
    "    \"\"\"计算每个标签的 AUC\"\"\"\n",
    "    AUROCs = []\n",
    "    gt_np = gt.cpu().numpy()\n",
    "    pred_np = pred.cpu().numpy()\n",
    "    for i in range(n_class):\n",
    "        if np.sum(gt_np[:, i]) == 0:  # 跳过全 0 的类别\n",
    "            AUROCs.append(np.nan)\n",
    "        else:\n",
    "            try:\n",
    "                AUROCs.append(roc_auc_score(gt_np[:, i], pred_np[:, i]))\n",
    "            except ValueError:\n",
    "                AUROCs.append(np.nan)  # 当 AUC 计算失败时，填充 NaN\n",
    "    return AUROCs\n",
    "\n",
    "def get_acc_and_auc():\n",
    "    # 读取数据\n",
    "    output_path = '../data/eval/test_prompt/Chest-X-ray_llava_val_ans.jsonl'\n",
    "    answers = [json.loads(line) for line in open(output_path)]\n",
    "\n",
    "    disease_list = ['fibrosis', 'edema', 'pneumothorax', 'cardiomegaly', 'atelectasis', 'nodule', 'emphysema', 'no finding', 'mass', 'pleural_thickening', 'effusion', 'infiltration', 'pneumonia', 'hernia', 'consolidation']\n",
    "    print(f\"Total number of answers: {len(answers)}\")\n",
    "    \n",
    "    # 手动映射两个列表中的疾病\n",
    "    # disease_mapping = {\n",
    "    #     'atelectasis': 'Atelectasis',\n",
    "    #     'cardiomegaly': 'Cardiomegaly',\n",
    "    #     'consolidation': 'Consolidation',\n",
    "    #     'edema': 'Edema',\n",
    "    #     'pneumothorax': 'Pneumothorax',\n",
    "    #     'effusion': 'Pleural Effusion',\n",
    "    #     'pneumonia': 'Pneumonia',\n",
    "    #     'no finding': 'Normal',\n",
    "    #     'pleural_thickening': 'Pleural Other',\n",
    "    #     'mass': 'Lung Lesion',\n",
    "    #     'nodule': 'Lung Lesion',\n",
    "    #     'infiltration': 'Lung Opacity',\n",
    "    #     'fibrosis': None,      # 未映射的疾病\n",
    "    #     'emphysema': None,     # 未映射的疾病\n",
    "    #     'hernia': None         # 未映射的疾病\n",
    "    # }\n",
    "\n",
    "    # 随机选择 1000 行\n",
    "    random.shuffle(answers)\n",
    "    selected_answers = answers[:1000]\n",
    "\n",
    "    # 初始化 ground truth (gt) 和 predictions (pred)\n",
    "    n_classes = len(disease_list)\n",
    "    gt = torch.zeros((len(selected_answers), n_classes), dtype=torch.float32)\n",
    "    pred = torch.zeros((len(selected_answers), n_classes), dtype=torch.float32)\n",
    "\n",
    "    error_count = 0\n",
    "    error_question_ids = []\n",
    "\n",
    "    # 遍历每个 answer，提取 labels 和预测类别\n",
    "    for idx, item in enumerate(selected_answers):\n",
    "        # 获取标签（label），labels 可能包含多个标签\n",
    "        labels = [\"-\".join(item[\"question_id\"].split(\"-\")[1:])]  # 获取 label\n",
    "        labels = [label.lower() for label in labels]\n",
    "\n",
    "        # 设置 ground truth\n",
    "        for label in labels:\n",
    "            if label in disease_list:\n",
    "                disease_idx = disease_list.index(label)\n",
    "                gt[idx, disease_idx] = 1  # 对应疾病的 ground truth 置为 1\n",
    "\n",
    "        # 获取预测的 text 并映射到疾病\n",
    "        text = item[\"text\"].strip().lower()\n",
    "\n",
    "        # 设置预测的类别，映射预测类别为 ground truth 中的疾病\n",
    "        for disease in disease_list:\n",
    "            # mapped_disease = disease_mapping.get(disease)\n",
    "            mapped_disease = disease\n",
    "            if mapped_disease and mapped_disease.lower() in text:  # 确保映射后的疾病出现在预测中\n",
    "                disease_idx = disease_list.index(disease)\n",
    "                pred[idx, disease_idx] = 1  # 对应疾病的预测值置为 1\n",
    "\n",
    "        # 检查是否没有预测到任何类别\n",
    "        if torch.sum(pred[idx]) == 0:\n",
    "            error_count += 1\n",
    "            error_question_ids.append(item[\"question_id\"])\n",
    "\n",
    "    # 计算多标签 AUC\n",
    "    AUROCs = compute_AUCs(gt, pred, n_classes)\n",
    "    AUROC_avg = np.nanmean(AUROCs)  # 计算 AUC 的平均值，忽略 NaN\n",
    "\n",
    "    # 计算每个类别的准确率\n",
    "    accs = []\n",
    "    gt_np_all = gt.cpu().numpy()\n",
    "    pred_np_all = pred.cpu().numpy()\n",
    "\n",
    "    for i in range(n_classes):\n",
    "        gt_np = gt_np_all[:, i]\n",
    "        pred_np = pred_np_all[:, i]\n",
    "        \n",
    "        if np.sum(gt_np) == 0:  # 跳过全 0 的类别\n",
    "            accs.append(np.nan)\n",
    "        else:\n",
    "            acc = accuracy_score(gt_np, pred_np)\n",
    "            accs.append(acc)\n",
    "    \n",
    "    for i, acc in enumerate(accs):\n",
    "        print(f\"{disease_list[i]}: {acc:.4f}\")\n",
    "    \n",
    "    # 计算类别平均准确率\n",
    "    acc_avg = np.nanmean(accs)\n",
    "\n",
    "    # 计算 F1 分数\n",
    "    f1s = []\n",
    "\n",
    "    for i in range(n_classes):\n",
    "        gt_np = gt_np_all[:, i]\n",
    "        pred_np = pred_np_all[:, i]\n",
    "\n",
    "        if np.sum(gt_np) == 0:  # 跳过全 0 的类别\n",
    "            f1s.append(np.nan)\n",
    "        else:\n",
    "            precision, recall, thresholds = precision_recall_curve(gt_np, pred_np)\n",
    "            if len(precision) > 1:\n",
    "                numerator = 2 * recall * precision\n",
    "                denom = recall + precision\n",
    "                f1_scores = np.divide(numerator, denom, out=np.zeros_like(denom), where=(denom != 0))\n",
    "                max_f1 = np.max(f1_scores)\n",
    "                f1s.append(max_f1)\n",
    "            else:\n",
    "                f1s.append(np.nan)\n",
    "\n",
    "    f1_avg = np.nanmean(f1s)  # 平均 F1\n",
    "    # 获取预测为 0 的类别索引\n",
    "    zero_pred_indices = np.where(np.sum(pred_np_all, axis=0) == 0)[0]\n",
    "\n",
    "    # 输出预测为 0 的类别\n",
    "    zero_pred_diseases = [disease_list[i] for i in zero_pred_indices]\n",
    "    print(f\"Predicted as 0 for the following diseases: {zero_pred_diseases}\")\n",
    "\n",
    "\n",
    "    # 输出结果\n",
    "    print(f\"Total labels: {len(selected_answers)}\")\n",
    "    print(f\"Class distribution in ground truth: {np.sum(gt_np_all, axis=0)}\")\n",
    "    print(f\"Class distribution in predictions: {np.sum(pred_np_all, axis=0)}\")\n",
    "    print(f\"Average AUROC: {AUROC_avg:.4f}\")\n",
    "    print(f\"Average F1: {f1_avg:.4f}\")\n",
    "    print(f\"Average Accuracy: {acc_avg:.4f}\")\n",
    "    print(f\"Number of errors: {error_count}\")\n",
    "    print(f\"Error question IDs: {error_question_ids}\")\n",
    "\n",
    "# 调用函数进行计算\n",
    "get_acc_and_auc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Few-shot dataset saved to ../data/eval/Chest-X-ray_few_shot.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "dataset = \"Chest-X-ray\"\n",
    "category_samples = defaultdict(list)\n",
    "\n",
    "# 从 JSONL 文件中加载数据\n",
    "data = [json.loads(line) for line in open(f\"../data/eval/{dataset}.jsonl\")]\n",
    "\n",
    "# 目标疾病列表\n",
    "disease_list = ['fibrosis', 'edema', 'pneumothorax', 'cardiomegaly', 'atelectasis', 'nodule', 'emphysema', 'no finding', 'mass', 'pleural_thickening', 'effusion', 'infiltration', 'pneumonia', 'hernia', 'consolidation']\n",
    "\n",
    "# 收集每个类别的样本\n",
    "for item in data:\n",
    "    labels = item[\"label\"]  # 假设每个数据项有一个 \"label\" 字段\n",
    "    split = item[\"split\"]  # 假设每个数据项有一个 \"split\" 字段\n",
    "    if isinstance(labels, list) and split == \"train\":  # 如果标签是列表形式，说明可能多标签分类\n",
    "        for label in labels:\n",
    "            if label in disease_list:\n",
    "                category_samples[label].append(item)\n",
    "    else:\n",
    "        if labels in disease_list and split == \"train\":\n",
    "            category_samples[labels].append(item)\n",
    "\n",
    "# 构建 few-shot 数据集，确保每个类别包含最多 10 个样本\n",
    "few_shot_dataset = []\n",
    "\n",
    "for category, samples in category_samples.items():\n",
    "    few_shot_samples = random.sample(samples, min(10, len(samples)))  # 随机选择最多 10 个样本\n",
    "    few_shot_dataset.extend(few_shot_samples)\n",
    "\n",
    "# 将结果转换为 DataFrame 或者直接输出\n",
    "df_few_shot = pd.DataFrame(few_shot_dataset)\n",
    "\n",
    "# 保存 few-shot 数据集到 JSONL 文件\n",
    "few_shot_file = f\"../data/eval/{dataset}_few_shot.jsonl\"\n",
    "df_few_shot.to_json(few_shot_file, orient='records', lines=True)\n",
    "\n",
    "print(f\"Few-shot dataset saved to {few_shot_file}\")\n",
    "\n",
    "# import json\n",
    "# import random\n",
    "# import pandas as pd\n",
    "# from collections import defaultdict\n",
    "\n",
    "# dataset = \"Chest-X-ray\"\n",
    "# category_samples = defaultdict(list)\n",
    "\n",
    "# # 从 JSONL 文件中加载数据\n",
    "# data = [json.loads(line) for line in open(f\"../data/eval/{dataset}.jsonl\")]\n",
    "\n",
    "# disease_list = ['fibrosis', 'edema', 'pneumothorax', 'cardiomegaly', 'atelectasis', 'nodule', 'emphysema', 'no finding', 'mass', 'pleural_thickening', 'effusion', 'infiltration', 'pneumonia', 'hernia', 'consolidation']\n",
    "\n",
    "\n",
    "# for item in data:\n",
    "#     labels = item[\"label\"]  # 假设每个数据项有一个 \"label\" 字段\n",
    "#     split = item[\"split\"]  # 假设每个数据项有一个 \"split\" 字段\n",
    "#     if isinstance(labels, list) and split == \"train\":  # 如果标签是列表形式，说明可能多标签分类\n",
    "#         for label in labels:\n",
    "#             category_samples[label].append(item)\n",
    "#     else:\n",
    "#         category_samples[labels].append(item)\n",
    "\n",
    "# # 构建 few-shot 数据集，确保每个类别包含 5 个样本\n",
    "# few_shot_dataset = []\n",
    "\n",
    "# for category, samples in category_samples.items():\n",
    "\n",
    "#     few_shot_samples = random.sample(samples, min(10, len(samples)))\n",
    "#     few_shot_dataset.extend(few_shot_samples)\n",
    "\n",
    "# # 将结果转换为 DataFrame 或者直接输出\n",
    "# df_few_shot = pd.DataFrame(few_shot_dataset)\n",
    "\n",
    "# # 输出结果\n",
    "# # print(df_few_shot)\n",
    "# # 保存 few-shot 数据集\n",
    "# few_shot_file = f\"../data/eval/{dataset}_few_shot.jsonl\"\n",
    "# df_few_shot.to_json(few_shot_file, orient='records', lines=True)\n",
    "\n",
    "# print(f\"Few-shot dataset saved to {few_shot_file}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a chest X-ray showing\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 假设你使用的是某个特定模型的 tokenizer，例如 'bert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained('/srv/lby/llava_med/llava-med-v1.5-mistral-7b')\n",
    "\n",
    "# 你的 token ids\n",
    "token_ids = [1, 851, 349, 264, 8118, 1500, 28733, 919, 8102]\n",
    "\n",
    "# 使用 tokenizer 的 decode 方法还原\n",
    "decoded_text = tokenizer.decode(token_ids, skip_special_tokens=True)\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted dataset saved to ../data/train/sft_data/Chest-X-ray_few_shot_formatted.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "# 创建一个空列表以存储结果\n",
    "formatted_data = []\n",
    "\n",
    "# 遍历 DataFrame 中的每一行\n",
    "for idx, row in df_few_shot.iterrows():\n",
    "    labels = row['label']\n",
    "    # 提取需要的字段\n",
    "    item = {\n",
    "        'id': idx,\n",
    "        'image': row['image'],  # 假设该列名为 'image'\n",
    "        'conversations': [\n",
    "            {\n",
    "                'from': 'human',\n",
    "                'value': \"<image>\\n Fill in the blank: this is a chest X-ray showing {}\"\n",
    "            },\n",
    "            {\n",
    "                'from': 'gpt',\n",
    "                'value': f\"This is a chest X-ray showing {labels}\"  # 假设该列名为 'label'，对应疾病名称\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # 将构建的字典添加到列表中\n",
    "    formatted_data.append(item)\n",
    "\n",
    "# 将结果保存为 JSON 格式\n",
    "output_json_path = \"../data/train/sft_data/Chest-X-ray_few_shot_formatted.json\"\n",
    "with open(output_json_path, 'w') as json_file:\n",
    "    json.dump(formatted_data, json_file, indent=4)\n",
    "\n",
    "print(f\"Formatted dataset saved to {output_json_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys in non_lora_trainables:\n",
      "base_model.model.model.mm_projector.0.weight\n",
      "base_model.model.model.mm_projector.0.bias\n",
      "base_model.model.model.mm_projector.2.weight\n",
      "base_model.model.model.mm_projector.2.bias\n",
      "base_model.model.mis_mlp.out_mlp.0.weight\n",
      "base_model.model.mis_mlp.out_mlp.0.bias\n",
      "base_model.model.mis_mlp.out_mlp.1.weight\n",
      "base_model.model.mis_mlp.out_mlp.1.bias\n",
      "base_model.model.mis_mlp.out_mlp.3.weight\n",
      "base_model.model.mis_mlp.out_mlp.3.bias\n",
      "\n",
      "Weight shapes:\n",
      "base_model.model.model.mm_projector.0.weight: torch.Size([4096, 1024])\n",
      "base_model.model.model.mm_projector.0.bias: torch.Size([4096])\n",
      "base_model.model.model.mm_projector.2.weight: torch.Size([4096, 4096])\n",
      "base_model.model.model.mm_projector.2.bias: torch.Size([4096])\n",
      "base_model.model.mis_mlp.out_mlp.0.weight: torch.Size([4096])\n",
      "base_model.model.mis_mlp.out_mlp.0.bias: torch.Size([4096])\n",
      "base_model.model.mis_mlp.out_mlp.1.weight: torch.Size([1024, 4096])\n",
      "base_model.model.mis_mlp.out_mlp.1.bias: torch.Size([1024])\n",
      "base_model.model.mis_mlp.out_mlp.3.weight: torch.Size([512, 1024])\n",
      "base_model.model.mis_mlp.out_mlp.3.bias: torch.Size([512])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "\n",
    "# 加载权重文件\n",
    "file_path = '/home/lby/llava_med/LLaVA-Med/llava/run/checkpoints/llava-lora-new-clip-v1/non_lora_trainables.bin'\n",
    "non_lora_trainables = torch.load(file_path, map_location='cpu')\n",
    "\n",
    "# 打印所有权重的键名（模块名称）\n",
    "print(\"Keys in non_lora_trainables:\")\n",
    "for key in non_lora_trainables.keys():\n",
    "    print(key)\n",
    "\n",
    "# 如果想统计权重的形状\n",
    "print(\"\\nWeight shapes:\")\n",
    "for key, value in non_lora_trainables.items():\n",
    "    print(f\"{key}: {value.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llava-med",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
